{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21788e6-13cc-48c6-9049-cd82c7c38e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query Engine for 10-K Filings Analysis\n",
    "Handles query processing and retrieval from vector store\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    \"\"\"Data class for query results\"\"\"\n",
    "    content: str\n",
    "    score: float\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "\n",
    "class QueryEngine:\n",
    "    \"\"\"\n",
    "    Handles query processing and retrieval from vector store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize query engine\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store instance (FAISS/Chroma)\n",
    "            embedding_model_name: Name of the embedding model\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "    def process_query(self, query: str, top_k: int = 5, \n",
    "                     score_threshold: float = 0.5) -> List[QueryResult]:\n",
    "        \"\"\"\n",
    "        Process a single query and return relevant results\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum score threshold for results\n",
    "            \n",
    "        Returns:\n",
    "            List of QueryResult objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_embedding = self.embedding_model.encode([query])\n",
    "            \n",
    "            # Search in vector store\n",
    "            results = self.vector_store.search(\n",
    "                query_embedding[0], \n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            # Filter and format results\n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                if result.get('score', 0) >= score_threshold:\n",
    "                    formatted_results.append(QueryResult(\n",
    "                        content=result.get('content', ''),\n",
    "                        score=result.get('score', 0),\n",
    "                        metadata=result.get('metadata', {}),\n",
    "                        source=result.get('source', 'unknown')\n",
    "                    ))\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(formatted_results)} results for query: {query[:50]}...\")\n",
    "            return formatted_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def multi_query_search(self, queries: List[str], \n",
    "                          top_k: int = 5) -> Dict[str, List[QueryResult]]:\n",
    "        \"\"\"\n",
    "        Process multiple queries and return results for each\n",
    "        \n",
    "        Args:\n",
    "            queries: List of search queries\n",
    "            top_k: Number of top results per query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping queries to their results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for query in queries:\n",
    "            results[query] = self.process_query(query, top_k)\n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, query: str, company_filter: Optional[str] = None,\n",
    "                       year_filter: Optional[str] = None, \n",
    "                       top_k: int = 10) -> List[QueryResult]:\n",
    "        \"\"\"\n",
    "        Perform semantic search with optional filters\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            company_filter: Filter by company name/ticker\n",
    "            year_filter: Filter by filing year\n",
    "            top_k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of filtered QueryResult objects\n",
    "        \"\"\"\n",
    "        # Get initial results\n",
    "        results = self.process_query(query, top_k * 2)  # Get more to allow for filtering\n",
    "        \n",
    "        # Apply filters\n",
    "        filtered_results = []\n",
    "        for result in results:\n",
    "            metadata = result.metadata\n",
    "            \n",
    "            # Apply company filter\n",
    "            if company_filter:\n",
    "                company_match = (\n",
    "                    company_filter.lower() in metadata.get('company', '').lower() or\n",
    "                    company_filter.lower() in metadata.get('ticker', '').lower()\n",
    "                )\n",
    "                if not company_match:\n",
    "                    continue\n",
    "            \n",
    "            # Apply year filter\n",
    "            if year_filter:\n",
    "                if str(year_filter) not in str(metadata.get('year', '')):\n",
    "                    continue\n",
    "            \n",
    "            filtered_results.append(result)\n",
    "            \n",
    "            # Stop when we have enough results\n",
    "            if len(filtered_results) >= top_k:\n",
    "                break\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    def get_context_window(self, query_result: QueryResult, \n",
    "                          window_size: int = 2) -> str:\n",
    "        \"\"\"\n",
    "        Get expanded context around a query result\n",
    "        \n",
    "        Args:\n",
    "            query_result: The original query result\n",
    "            window_size: Number of chunks before/after to include\n",
    "            \n",
    "        Returns:\n",
    "            Expanded context string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            chunk_id = query_result.metadata.get('chunk_id')\n",
    "            if not chunk_id:\n",
    "                return query_result.content\n",
    "            \n",
    "            # Get surrounding chunks\n",
    "            context_chunks = self.vector_store.get_surrounding_chunks(\n",
    "                chunk_id, window_size\n",
    "            )\n",
    "            \n",
    "            return \" \".join(context_chunks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not get context window: {str(e)}\")\n",
    "            return query_result.content\n",
    "    \n",
    "    def rank_results_by_relevance(self, results: List[QueryResult], \n",
    "                                 query: str) -> List[QueryResult]:\n",
    "        \"\"\"\n",
    "        Re-rank results by relevance to the original query\n",
    "        \n",
    "        Args:\n",
    "            results: List of query results\n",
    "            query: Original query\n",
    "            \n",
    "        Returns:\n",
    "            Re-ranked list of results\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_model.encode([query])[0]\n",
    "            \n",
    "            # Re-calculate scores\n",
    "            for result in results:\n",
    "                content_embedding = self.embedding_model.encode([result.content])[0]\n",
    "                similarity = np.dot(query_embedding, content_embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(content_embedding)\n",
    "                )\n",
    "                result.score = float(similarity)\n",
    "            \n",
    "            # Sort by score\n",
    "            return sorted(results, key=lambda x: x.score, reverse=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not re-rank results: {str(e)}\")\n",
    "            return results\n",
    "    \n",
    "    def extract_key_phrases(self, text: str, max_phrases: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key phrases from text for query expansion\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            max_phrases: Maximum number of phrases to extract\n",
    "            \n",
    "        Returns:\n",
    "            List of key phrases\n",
    "        \"\"\"\n",
    "        # Simple implementation - can be enhanced with NLP libraries\n",
    "        import re\n",
    "        \n",
    "        # Remove common stop words and extract meaningful phrases\n",
    "        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'a', 'an'}\n",
    "        \n",
    "        # Split into sentences and extract noun phrases\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        phrases = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower())\n",
    "            # Create 2-3 word phrases\n",
    "            for i in range(len(words) - 1):\n",
    "                if words[i] not in stop_words and words[i+1] not in stop_words:\n",
    "                    phrase = f\"{words[i]} {words[i+1]}\"\n",
    "                    if len(phrase) > 4:  # Minimum phrase length\n",
    "                        phrases.append(phrase)\n",
    "        \n",
    "        # Return most frequent phrases\n",
    "        phrase_counts = {}\n",
    "        for phrase in phrases:\n",
    "            phrase_counts[phrase] = phrase_counts.get(phrase, 0) + 1\n",
    "        \n",
    "        return sorted(phrase_counts.keys(), key=phrase_counts.get, reverse=True)[:max_phrases]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
