{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b701f7-33be-4ab5-9fb1-c1087a487d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from groq import Groq \n",
    "# Configuration & Setup\n",
    "EMBEDDED_CHUNKS_FILE = Path(\"data/embedded_chunks.json\")\n",
    "\n",
    "# API KEY FOR GROQ\n",
    "\n",
    "client = Groq(api_key=\"replace your key here\") # Make sure your actual key is here\n",
    "\n",
    "# Load Data\n",
    "def load_embedded_chunks():\n",
    "    \"\"\"Loads embedded chunks from the JSON file.\"\"\"\n",
    "    if not EMBEDDED_CHUNKS_FILE.exists():\n",
    "        print(f\"Error: Embedded chunks file not found at {EMBEDDED_CHUNKS_FILE}\")\n",
    "        return None\n",
    "    with open(EMBEDDED_CHUNKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Semantic Search\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "except ImportError:\n",
    "    print(\"SentenceTransformer not found. Please ensure it's installed (pip install sentence-transformers).\")\n",
    "    print(\"Assuming `embed_model` is provided by the execution environment or `embedder.ipynb` has been run.\")\n",
    "    embed_model = None\n",
    "\n",
    "def semantic_search(query_embedding, embedded_data, top_k=15):\n",
    "    \"\"\"Performs semantic search on embedded chunks using a query embedding.\"\"\"\n",
    "    if embedded_data is None:\n",
    "        return []\n",
    "\n",
    "    if embed_model is None:\n",
    "        print(\" Error: Embedding model not initialized. Cannot perform semantic search.\")\n",
    "        return []\n",
    "\n",
    "    similarities = []\n",
    "    for chunk in embedded_data:\n",
    "        if \"embedding\" in chunk and chunk[\"embedding\"] is not None:\n",
    "            chunk_embedding = np.array(chunk[\"embedding\"])\n",
    "\n",
    "            if query_embedding.ndim == 1:\n",
    "                q_emb = query_embedding.reshape(1, -1)\n",
    "            else:\n",
    "                q_emb = query_embedding\n",
    "\n",
    "            if chunk_embedding.ndim == 1:\n",
    "                c_emb = chunk_embedding.reshape(1, -1)\n",
    "            else:\n",
    "                c_emb = chunk_embedding\n",
    "\n",
    "            score = cosine_similarity(q_emb, c_emb)[0][0]\n",
    "            similarities.append((chunk, score))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Query Decomposition Function\n",
    "def decompose_query(user_query, llm_client):\n",
    "    \"\"\"Uses LLM to decompose a complex query into simpler sub-queries.\"\"\"\n",
    "    prompt_decompose = f\"\"\"\n",
    "    You are a financial query analyzer. Your task is to analyze a user's question about financial filings\n",
    "    and determine if it requires decomposition into multiple sub-questions.\n",
    "\n",
    "    If the question is simple and can be answered with a single direct search (e.g., \"What was Microsoft's revenue in 2023?\"),\n",
    "    return only the original question in a JSON object with the key \"sub_queries\".\n",
    "\n",
    "    If the question is complex, comparative, or requires information from multiple companies/years\n",
    "    (e.g., \"How did NVIDIA's data center revenue grow from 2022 to 2023?\", \"Which company had the highest operating margin in 2023?\"),\n",
    "    break it down into specific, atomic sub-questions. Each sub-question should be answerable by a single retrieval.\n",
    "\n",
    "    Consider company names (Microsoft, Google, NVIDIA) and fiscal years (e.g., 2022, 2023, 2024) for decomposition.\n",
    "\n",
    "    Format your output strictly as a JSON object with a single key \"sub_queries\" whose value is a JSON list of strings.\n",
    "    Do NOT include any conversational text, preamble, or any other keys. Only the JSON object.\n",
    "\n",
    "    Example 1 (Simple):\n",
    "    Question: What was Google's total revenue in 2023?\n",
    "    Output: {{\"sub_queries\": [\"What was Google's total revenue in 2023?\"]}}\n",
    "\n",
    "    Example 2 (Comparative):\n",
    "    Question: How did NVIDIA's data center revenue grow from 2022 to 2023?\n",
    "    Output: {{\"sub_queries\": [\"NVIDIA data center revenue 2022\", \"NVIDIA data center revenue 2023\"]}}\n",
    "\n",
    "    Example 3 (Cross-Company):\n",
    "    Question: Which company had the highest operating margin in 2023?\n",
    "    Output: {{\"sub_queries\": [\"Microsoft operating margin 2023\", \"Google operating margin 2023\", \"NVIDIA operating margin 2023\"]}}\n",
    "\n",
    "    Example 4 (Complex Multi-aspect):\n",
    "    Question: Compare cloud revenue growth rates across all three companies from 2022 to 2023\n",
    "    Output: {{\n",
    "        \"sub_queries\": [\n",
    "            \"Microsoft cloud revenue 2022\",\n",
    "            \"Microsoft cloud revenue 2023\",\n",
    "            \"Google cloud revenue 2022\",\n",
    "            \"Google cloud revenue 2023\",\n",
    "            \"NVIDIA data center revenue 2022\", # NVIDIA often reports data center revenue instead of \"cloud\"\n",
    "            \"NVIDIA data center revenue 2023\"\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Question: {user_query}\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_decompose}],\n",
    "            model=\"llama3-8b-8192\", # Using a smaller model for decomposition might be faster\n",
    "            response_format={\"type\": \"json_object\"} # Re-added this\n",
    "        )\n",
    "        sub_queries_raw_str = response.choices[0].message.content\n",
    "        parsed_output = json.loads(sub_queries_raw_str)\n",
    "\n",
    "        # Now we strictly expect a dictionary with a \"sub_queries\" key\n",
    "        if isinstance(parsed_output, dict) and \"sub_queries\" in parsed_output and isinstance(parsed_output[\"sub_queries\"], list):\n",
    "            sub_queries = parsed_output[\"sub_queries\"]\n",
    "        else:\n",
    "            raise ValueError(f\"LLM did not return a JSON object with 'sub_queries' key as expected. Got: {parsed_output}\")\n",
    "\n",
    "        if not sub_queries: # If list is empty after parsing\n",
    "            raise ValueError(\"No sub-queries extracted from LLM response.\")\n",
    "\n",
    "        return sub_queries\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error, LLM output might not be valid JSON: {sub_queries_raw_str}. Error: {e}\")\n",
    "        return [user_query] # Fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error decomposing query, defaulting to original query: {e}\")\n",
    "        return [user_query] # Fallback\n",
    "\n",
    "# Agent Query Engine\n",
    "def agent_query(user_query, top_k_per_subquery=3):\n",
    "    \"\"\"\n",
    "    Main function for the RAG agent to answer a query.\n",
    "    1. Decomposes complex queries into sub-queries.\n",
    "    2. Performs multi-step semantic search for each sub-query.\n",
    "    3. Constructs a prompt with the user query and combined retrieved context.\n",
    "    4. Uses a generative model (Groq) to synthesize an answer.\n",
    "    \"\"\"\n",
    "\n",
    "    embedded_chunks = load_embedded_chunks()\n",
    "    if embedded_chunks is None:\n",
    "        return {\"query\": user_query, \"answer\": \"Could not load embedded chunks.\", \"reasoning\": \"\", \"sub_queries\": [], \"sources\": []}\n",
    "\n",
    "    all_retrieved_chunks_with_scores = []\n",
    "    reasoning_steps = []\n",
    "    final_sub_queries = [] # To store the actual sub-queries used\n",
    "\n",
    "    # 1. Query Decomposition\n",
    "    sub_queries = decompose_query(user_query, client)\n",
    "    final_sub_queries.extend(sub_queries)\n",
    "\n",
    "    print(f\"\\n--- Decomposed Sub-queries for '{user_query}': ---\")\n",
    "    for sq_idx, sq_val in enumerate(sub_queries):\n",
    "        print(f\"  {sq_idx + 1}. {sq_val}\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    if len(sub_queries) > 1:\n",
    "        reasoning_steps.append(f\"Original query decomposed into {len(sub_queries)} sub-queries.\")\n",
    "    else:\n",
    "        reasoning_steps.append(\"Original query processed as a single search.\")\n",
    "\n",
    "    # 2. Multi-step Retrieval\n",
    "    for sq in sub_queries:\n",
    "        reasoning_steps.append(f\"Retrieving information for sub-query: '{sq}'\")\n",
    "        if embed_model is None:\n",
    "            reasoning_steps.append(\"Embedding model not available, skipping semantic search for sub-query.\")\n",
    "            continue\n",
    "        sq_embedding = embed_model.encode(sq)\n",
    "        sq_top_results = semantic_search(sq_embedding, embedded_chunks, top_k=top_k_per_subquery)\n",
    "        all_retrieved_chunks_with_scores.extend(sq_top_results)\n",
    "\n",
    "    print(\"\\n--- All Retrieved Chunks (before deduplication): ---\")\n",
    "    if all_retrieved_chunks_with_scores:\n",
    "        for chunk_score_pair in all_retrieved_chunks_with_scores:\n",
    "            chunk = chunk_score_pair[0]\n",
    "            score = chunk_score_pair[1]\n",
    "            print(f\"  Company: {chunk.get('company', 'N/A')}, Year: {chunk.get('year', 'N/A')}, Score: {score:.4f}, Excerpt: {chunk.get('text', '')[:100]}...\")\n",
    "    else:\n",
    "        print(\"  No chunks retrieved for any sub-query.\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "    if not all_retrieved_chunks_with_scores:\n",
    "        return {\"query\": user_query, \"answer\": \"No relevant chunks found across all sub-queries.\", \"reasoning\": \"Semantic search returned no results.\", \"sub_queries\": final_sub_queries, \"sources\": []}\n",
    "\n",
    "    # Deduplicate and sort collected chunks by score (highest first)\n",
    "    # Use a set to track unique chunk texts to avoid redundancy\n",
    "    unique_chunks_map = {}\n",
    "    for chunk, score in all_retrieved_chunks_with_scores:\n",
    "        # Use a unique identifier for the chunk, e.g., combination of company, year, and text hash\n",
    "        chunk_id = (chunk.get(\"company\"), chunk.get(\"year\"), hash(chunk.get(\"text\", \"\")))\n",
    "        if chunk_id not in unique_chunks_map or score > unique_chunks_map[chunk_id][1]:\n",
    "            unique_chunks_map[chunk_id] = (chunk, score)\n",
    "\n",
    "    deduplicated_top_results = sorted(unique_chunks_map.values(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Prepare context for the LLM from deduplicated results\n",
    "    top_chunks_text = [chunk.get(\"text\", \"\") for chunk, _ in deduplicated_top_results]\n",
    "    context = \"\\n\\n---\\n\\n\".join(top_chunks_text)\n",
    "\n",
    "    # Prepare sources for attribution (using .get() for safety)\n",
    "    sources = [{\n",
    "        \"company\": chunk.get(\"company\", \"N/A\"),\n",
    "        \"year\": chunk.get(\"year\", \"N/A\"),\n",
    "        \"fiscal_year\": chunk.get(\"fiscal_year\", \"N/A\"), # Include fiscal_year if available\n",
    "        \"excerpt\": chunk.get(\"text\", \"\")[:300] + (\"...\" if len(chunk.get(\"text\", \"\")) > 300 else \"\"),\n",
    "        \"score\": float(score)\n",
    "    } for chunk, score in deduplicated_top_results]\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an AI financial analyst. Answer the user's question precisely based on the provided context from 10-K filings.\n",
    "    If the question involves comparison or calculation, perform it accurately and show your work if appropriate.\n",
    "    State any numerical values clearly, including units (e.g., billions, millions) and percentages.\n",
    "    If information for a specific company or year is not available in the context, state that clearly for that specific entity.\n",
    "    Do not make up information. If you cannot answer the question from the provided context, state that you do not have enough information.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Question: {user_query}\n",
    "\n",
    "    Context from 10-K filings:\n",
    "    {context}\n",
    "\n",
    "    Synthesize a comprehensive answer based ONLY on the provided context.\n",
    "    If the question involved comparing multiple entities or periods, provide a clear and concise comparison, including specific figures and growth rates if found.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=1000, # Increase max_tokens for comprehensive answers\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content.strip()\n",
    "        reasoning_steps.append(\"Used Groq model to synthesize answer from combined retrieved context.\")\n",
    "    except Exception as e:\n",
    "        answer = \"Could not generate answer due to LLM error: \" + str(e) + \". Please check your Groq API key and model access.\"\n",
    "        reasoning_steps.append(f\"LLM synthesis failed: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"answer\": answer,\n",
    "        \"reasoning\": \"\\n\".join(reasoning_steps),\n",
    "        \"sub_queries\": final_sub_queries, # Include the generated sub-queries\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import pprint # Import pprint here if not at the top\n",
    "\n",
    "    print(\"\\n--- Testing with a simple query ---\")\n",
    "    query_simple = \"What was Microsoft's total revenue in 2023?\"\n",
    "    try:\n",
    "        response_simple = agent_query(query_simple)\n",
    "        pprint.pprint(response_simple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during simple query execution: {e}\")\n",
    "\n",
    "    print(\"\\n--- Testing with a comparative query (YoY growth) ---\")\n",
    "    query_yoy = \"How did NVIDIA's data center revenue grow from 2022 to 2023?\"\n",
    "    try:\n",
    "        response_yoy = agent_query(query_yoy)\n",
    "        pprint.pprint(response_yoy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during comparative query execution: {e}\")\n",
    "\n",
    "    print(\"\\n--- Testing with a complex cross-company/segment query ---\")\n",
    "    query_complex = \"Compare cloud revenue growth rates across all three companies (Google, Microsoft, NVIDIA) from 2022 to 2023.\"\n",
    "    try:\n",
    "        response_complex = agent_query(query_complex)\n",
    "        pprint.pprint(response_complex)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during complex query execution: {e}\")\n",
    "\n",
    "    print(\"\\n--- Testing with a segment analysis query ---\")\n",
    "    query_segment_analysis = \"What percentage of Google's revenue came from cloud in 2023?\"\n",
    "    try:\n",
    "        response_segment_analysis = agent_query(query_segment_analysis)\n",
    "        pprint.pprint(response_segment_analysis)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during segment analysis query execution: {e}\")\n",
    "\n",
    "    print(\"\\n--- Testing with an AI Strategy query ---\")\n",
    "    query_ai_strategy = \"Compare AI investments mentioned by all three companies (Google, Microsoft, NVIDIA) in their 2024 10-Ks.\"\n",
    "    try:\n",
    "        response_ai_strategy = agent_query(query_ai_strategy)\n",
    "        pprint.pprint(response_ai_strategy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during AI strategy query execution: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
